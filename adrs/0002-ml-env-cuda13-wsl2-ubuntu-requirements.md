# ADR 0002: Environment Setup and Requirements Management

## Status
Accepted

## Context
The ML-Env-CUDA13 repository provides automated setup scripts and environment-specific requirements files for both Windows and WSL2/Ubuntu. Full GPU support for both PyTorch and TensorFlow is only available in WSL2/Ubuntu. The setup scripts generate reproducible requirements files (`requirements.txt` for Windows, `requirements-wsl.txt` for WSL2) to ensure compatibility and ease of environment recreation.

## Common Python Library Requirements for Fine-Tuning
To fine-tune LLMs and run inference, the following Python libraries and tools are required:
- transformers (Hugging Face)
- peft (Parameter-Efficient Fine-Tuning, e.g., LoRA adapters)
- bitsandbytes (quantization, 4/8-bit training)
- accelerate (multi-GPU, distributed training)
- sentencepiece (tokenization)
- torch (PyTorch)
- huggingface_hub (model download/upload)
- datasets (Hugging Face datasets)
- trl (SFTTrainer, RLHF workflows)
- deepspeed (optional, distributed training)
- scikit-learn (optional, data preprocessing)
- numpy (core scientific computing)
- tqdm (progress bars)
- unsloth (efficient LLM fine-tuning)
- gguf (for exporting models to GGUF format)

## Model Export and Adapter Support
- LoRA adapters: Used for parameter-efficient fine-tuning (via peft or unsloth)
- GGUF: Final models may be exported to GGUF format for compatibility with tools like llama.cpp and other inference engines
- llama.cpp: A popular C++ inference engine for running GGUF models efficiently on CPU and GPU. After fine-tuning and exporting to GGUF, use llama.cpp for fast, portable inference.

These libraries and tools should be installed in the ML-Env-CUDA13 Python environment using the requirements file or install scripts provided in this project.

## Decision
- All deep learning workflows (training, inference, fine-tuning) in this project will be performed in WSL2/Ubuntu for full GPU support.
- The ML-Env-CUDA13 setup script (`setup_ml_env_wsl.sh`) must be run before any fine-tuning or inference scripts in this project.
- The Python environment and dependencies will be managed using the requirements file generated by ML-Env-CUDA13 (`requirements-wsl.txt`).
- Users must follow the ML-Env-CUDA13 documentation for initial setup, including enabling WSL2, installing Ubuntu, and updating the system.

## Consequences
- Windows native environment is not supported for TensorFlow GPU workflows in this project.
- All documentation and scripts will reference WSL2/Ubuntu and the ML-Env-CUDA13 requirements file.
- Troubleshooting steps for CUDA, Python, and ML library issues will be based on the ML-Env-CUDA13 guidance.

## Alternatives Considered
- Windows native setup (rejected due to TensorFlow GPU limitations and reproducibility concerns)
- Docker-based environment (not chosen for simplicity and direct GPU access in WSL2)

---
